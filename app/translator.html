<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Translator | HandWave</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../assets/css/style.css">
</head>
<body class="light-mode">
  <div class="app-layout">

    <!-- Sidebar Navigation -->
    <nav class="sidebar">
      <a href="../index.html" class="logo text-decoration-none">
        <i class="fa-solid fa-hands-asl-interpreting"></i> HandWave
      </a>

      <div class="navigation flex-grow-1">
        <div class="nav flex-column">
          <a class="nav-link active" href="translator.html"><i class="fas fa-language nav-icon"></i> Translator</a>
          <a class="nav-link" href="dictionary.html"><i class="fas fa-book nav-icon"></i> Dictionary</a>
          <a class="nav-link" href="learn.html"><i class="fas fa-graduation-cap nav-icon"></i> Learn</a>
          <a class="nav-link" href="history.html"><i class="fas fa-history nav-icon"></i> History</a>
          <a class="nav-link" href="settings.html"><i class="fas fa-cog nav-icon"></i> Settings</a>
        </div>
      </div>

      <div class="sidebar-footer">
        <p class="text-muted small mb-0 text-center">Â© 2026 HandWave Project</p>
      </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
      <h1 class="mb-4 fw-bold">Real-Time Translator</h1>

      <div class="row g-4">
        <div class="col-lg-7">
          <div id="video-container" class="camera-placeholder position-relative">
            <div class="camera-placeholder">
                <i class="fas fa-video fa-3x mb-3"></i>
                <p>Camera feed will appear here.</p>
                <small class="text-muted">Click Start Translation to begin.</small>
            </div>
          </div>

          <!-- Start/Stop Translation Button -->
          <div class="d-flex justify-content-center mt-3">
            <button id="startBtn" class="btn btn-primary btn-lg" disabled>Loading Model...</button>
          </div>
        </div>

        <div class="col-lg-5">
          <div class="d-flex justify-content-between align-items-center mb-3">
            <h5 class="mb-0">Translation Output</h5>
            <div>
              <button id="speakBtn" class="btn btn-outline-secondary btn-sm me-2">
                <i class="fas fa-volume-up"></i>
              </button>
              <button id="copyBtn" class="btn btn-outline-secondary btn-sm">
                <i class="fas fa-copy"></i>
              </button>
            </div>
          </div>
          <div class="translation-output">
            <p id="translationText">...</p>
          </div>
          <h5 class="mt-4 mb-3">Suggestions</h5>
          <div>
            <span class="badge bg-secondary me-2 p-2">Hello</span>
            <span class="badge bg-secondary me-2 p-2">Help</span>
            <span class="badge bg-secondary me-2 p-2">Thank you</span>
          </div>
        </div>
      </div>
    </main>
  </div>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.12.0/dist/tf.min.js"></script>
  
  <!-- This line links the global script for theme persistence and navigation -->
  <script src="../assets/js/main.js"></script>
  
  <script type="module">
    import { HandLandmarker, FilesetResolver, DrawingUtils } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js";

    const startBtn = document.getElementById("startBtn");
    const videoContainer = document.getElementById("video-container");
    const translationText = document.getElementById("translationText");
    const speakBtn = document.getElementById("speakBtn");
    const copyBtn = document.getElementById("copyBtn");

    let handLandmarker = null;
    let webcamRunning = false;
    let stream = null;
    let video = null;
    let canvas = null;
    let canvasCtx = null;
    let lastVideoTime = -1;

    // Optional TFJS classifier (for FSL model)
    let fslModel = null;
    let labelMap = null; // array of labels in order the model outputs
    const FSL_MODEL_PATH = "../assets/model/fsl_model_js/model.json"; // change if your path differs
    const LABELS_JSON = "../assets/model/fsl_model_js/labels.json"; // optional mapping file

    // ================= LOAD HAND LANDMARKER =================
    async function createHandLandmarker() {
      const visionFileset = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );

      handLandmarker = await HandLandmarker.createFromOptions(visionFileset, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
          delegate: "GPU"
        },
        runningMode: "VIDEO",
        numHands: 2
      });

      console.log("HandLandmarker model loaded.");
      startBtn.disabled = false;
      startBtn.innerHTML = `<i class="fas fa-play-circle me-2"></i> Start Translation`;
    }

    // ================= LOAD FSL MODEL =================
    async function tryLoadFSLModel() {
      if (typeof tf === "undefined") {
        console.warn("TFJS not loaded; skipping FSL classifier load.");
        return;
      }

      try {
        // attempt to fetch model JSON
        const resp = await fetch(FSL_MODEL_PATH, { method: "HEAD" });
        if (!resp.ok) {
          console.info("No TFJS model found at", FSL_MODEL_PATH);
          return;
        }
      } catch (err) {
        // HEAD request failed (likely file not present)
        console.info("FSL model not found (HEAD check); skipping.", err);
        return;
      }

      try {
        translationText.innerText = "Loading FSL classifier...";
        fslModel = await tf.loadGraphModel(FSL_MODEL_PATH);
        console.log("FSL classifier loaded:", fslModel);

        // try to load labels.json if available
        try {
          const r = await fetch(LABELS_JSON);
          if (r.ok) {
            labelMap = await r.json();
            console.log("Label map loaded:", labelMap);
          } else {
            labelMap = null;
          }
        } catch (e) {
          labelMap = null;
        }

        // fallback: if no labels.json and number of outputs is known, create A-Z or generic indices
        if (!labelMap && fslModel && fslModel.outputs && fslModel.outputs.length === 1) {
          // attempt to infer number of classes from model output shape (only works if available)
          const outShape = fslModel.outputs[0].shape; // e.g. [null, 26]
          const classes = (outShape && outShape.length >= 2) ? outShape[1] : null;
          if (classes) {
            labelMap = Array.from({ length: classes }, (_, i) => String.fromCharCode(65 + i)); // A,B,C,...
          }
        }

        translationText.innerText = "...";
      } catch (err) {
        console.warn("Failed to load TFJS FSL model:", err);
        fslModel = null;
        labelMap = null;
      }
    }

    // ================= ENABLE CAMERA =================
    async function enableCam() {
      if (!handLandmarker) return console.log("Wait! Model not loaded yet.");

      if (webcamRunning) {
        webcamRunning = false;
        if (stream) stream.getTracks().forEach(track => track.stop());

        videoContainer.innerHTML = `
          <div class="camera-placeholder">
              <i class="fas fa-video fa-3x mb-3"></i>
              <p>Camera feed will appear here.</p>
              <small class="text-muted">Click Start Translation to begin.</small>
          </div>
        `;
        startBtn.innerText = "Start Translation";
        translationText.innerText = "...";
      } else {
        webcamRunning = true;
        startBtn.innerText = "Stop Translation";

        video = document.createElement("video");
        video.classList.add("video-view", "mirrored-video");
        video.autoplay = true;
        video.playsInline = true;

        canvas = document.createElement("canvas");
        canvas.classList.add("canvas-view");
        canvasCtx = canvas.getContext("2d");

        videoContainer.innerHTML = "";
        videoContainer.appendChild(video);
        videoContainer.appendChild(canvas);

        try {
          stream = await navigator.mediaDevices.getUserMedia({ video: true });
          video.srcObject = stream;
          video.addEventListener("loadeddata", predictWebcam);
        } catch (err) {
          console.error("Error accessing webcam:", err);
          alert("Camera access blocked or unavailable!");
          webcamRunning = false;
          startBtn.innerText = "Start Translation";
        }
      }
    }

    // ================= PREDICT HANDS =================
    async function predictWebcam() {
      if (!webcamRunning || !video || video.readyState < 2) return;

      const videoWidth = video.videoWidth;
      const videoHeight = video.videoHeight;
      canvas.width = videoWidth;
      canvas.height = videoHeight;

      const startTimeMs = performance.now();
      if (lastVideoTime !== video.currentTime) {
        lastVideoTime = video.currentTime;

        try {
          const results = await handLandmarker.detectForVideo(video, startTimeMs);

          canvasCtx.save();
          canvasCtx.clearRect(0, 0, canvas.width, canvas.height);

          if (results.landmarks && results.landmarks.length > 0) {
            const drawingUtils = new DrawingUtils(canvasCtx);
            for (const landmarks of results.landmarks) {
              drawingUtils.drawConnectors(
                landmarks,
                HandLandmarker.HAND_CONNECTIONS,
                { color: "#FFFFFF", lineWidth: 5 }
              );
              drawingUtils.drawLandmarks(landmarks, { color: "#4F46E5", lineWidth: 2 });
            }

            // If we have a TFJS classifier loaded, prepare input and run prediction
            if (fslModel && labelMap) {
              // Use first detected hand for single-letter detection
              const first = results.landmarks[0]; // array of 21 {x,y,z}

              // Get bounding box from landmarks
              let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
              for (const lm of first) {
                minX = Math.min(minX, lm.x);
                minY = Math.min(minY, lm.y);
                maxX = Math.max(maxX, lm.x);
                maxY = Math.max(maxY, lm.y);
              }

              // Expand bounding box slightly
              const margin = 0.1;
              const width = maxX - minX;
              const height = maxY - minY;
              minX -= width * margin;
              minY -= height * margin;
              maxX += width * margin;
              maxY += height * margin;

              // Clamp to [0,1]
              minX = Math.max(0, minX);
              minY = Math.max(0, minY);
              maxX = Math.min(1, maxX);
              maxY = Math.min(1, maxY);

              // Crop and resize image
              const imgWidth = video.videoWidth;
              const imgHeight = video.videoHeight;
              const cropX = minX * imgWidth;
              const cropY = minY * imgHeight;
              const cropW = (maxX - minX) * imgWidth;
              const cropH = (maxY - minY) * imgHeight;

              // Create temp canvas for cropping
              const tempCanvas = document.createElement('canvas');
              tempCanvas.width = cropW;
              tempCanvas.height = cropH;
              const tempCtx = tempCanvas.getContext('2d');
              tempCtx.drawImage(video, cropX, cropY, cropW, cropH, 0, 0, cropW, cropH);

              // Resize to 64x64
              const resizedCanvas = document.createElement('canvas');
              resizedCanvas.width = 64;
              resizedCanvas.height = 64;
              const resizedCtx = resizedCanvas.getContext('2d');
              resizedCtx.drawImage(tempCanvas, 0, 0, 64, 64);

              // Convert to tensor and normalize
              const imgTensor = tf.browser.fromPixels(resizedCanvas).toFloat().div(255).expandDims(0);

              // Predict
              const pred = fslModel.predict(imgTensor);
              const probs = pred.arraySync()[0];

              // Find best prediction
              let bestIdx = 0;
              let bestVal = probs[0];
              for (let i = 1; i < probs.length; i++) {
                if (probs[i] > bestVal) {
                  bestVal = probs[i];
                  bestIdx = i;
                }
              }

              const label = labelMap[bestIdx] || `Class ${bestIdx}`;
              const confidence = (bestVal * 100).toFixed(1);
              translationText.innerText = `${label} (${confidence}%)`;

              // Clean up tensors
              imgTensor.dispose();
              pred.dispose();
            } else {
              // If classifier not available, show simple status
              translationText.innerText = "Hand Detected!";
            }

          } else {
            translationText.innerText = "No hand detected...";
          }

          canvasCtx.restore();
        } catch (err) {
          console.error("Hand detection error:", err);
          translationText.innerText = "Detection failed!";
        }
      }

      if (webcamRunning) {
        window.requestAnimationFrame(predictWebcam);
      }
    }

    // ================= BUTTON EVENTS =================
    startBtn.addEventListener("click", enableCam);

    speakBtn.addEventListener("click", () => {
      const text = translationText.innerText;
      if (!text || text === "..." || text === "No hand detected...") return;
      const utterance = new SpeechSynthesisUtterance(text);
      speechSynthesis.speak(utterance);
    });

    copyBtn.addEventListener("click", () => {
      const text = translationText.innerText;
      if (!text || text === "..." || text === "No hand detected...") return;
      navigator.clipboard.writeText(text)
        .then(() => alert("Copied to clipboard!"))
        .catch(err => console.error("Copy failed", err));
    });

    // ================= INITIALIZE =================
    createHandLandmarker();
    tryLoadFSLModel();
  </script>
</body>
</html>